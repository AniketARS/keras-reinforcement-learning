{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGredientAgent:\n",
    "    \n",
    "    def __init__(self, n_features, n_actions, gamma, learning_rate):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%S%M\")\n",
    "        rewards_log_dir = 'logs/pg_agent/cartpole/' + current_time + '/rewards'\n",
    "        self.rewards_summary_writer = tf.summary.create_file_writer(rewards_log_dir)\n",
    "        self.rewards_summary = keras.metrics.Mean('rewards', dtype=tf.float32)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        self._build_net()\n",
    "    \n",
    "    def _build_net(self):\n",
    "        model_input = keras.Input(shape=(self.n_features,), name='model_input')\n",
    "        x = keras.layers.Dense(32, activation='relu')(model_input)\n",
    "        x = keras.layers.Dense(32, activation='relu')(x)\n",
    "        model_output = keras.layers.Dense(self.n_actions, activation='softmax' , name='model_output')(x)\n",
    "        self.model = keras.Model(model_input, model_output)\n",
    "        self.optimizer = keras.optimizers.Adam(self.learning_rate)\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        obs = obs[np.newaxis, :]\n",
    "        probs = self.model(obs)[0]\n",
    "        action = np.random.choice(self.n_actions, p=probs)\n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, obs, a, r):\n",
    "        self.states.append(obs)\n",
    "        self.actions.append(a)\n",
    "        self.rewards.append(r)\n",
    "            \n",
    "    def train(self):\n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        rewards = np.array(self.rewards)\n",
    "        \n",
    "        des_rew = np.zeros_like(rewards, dtype=np.float32)\n",
    "        for t in range(len(self.rewards)):\n",
    "            cur_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(rewards)):\n",
    "                cur_sum += rewards[k]*discount\n",
    "                discount *= self.gamma\n",
    "            des_rew[t] = cur_sum\n",
    "        mean = des_rew.mean()\n",
    "        std = des_rew.std() if des_rew.std() > 0 else 1\n",
    "        des_rew = (des_rew - mean) / std\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            neg_log_probs = keras.losses.sparse_categorical_crossentropy(y_true=actions, y_pred=self.model(states))\n",
    "            loss = neg_log_probs * des_rew\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def load(self):\n",
    "        self.model.load_weights('./model/pg_agent_cartpole.h5')\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save_weights('./model/pg_agent_cartpole.h5')\n",
    "    \n",
    "    def write_rewards(self, episode, total_reward):\n",
    "        self.rewards_summary(total_reward)\n",
    "        with self.rewards_summary_writer.as_default():\n",
    "            tf.summary.scalar('rewards', self.rewards_summary.result(), step=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "pg_agent = PolicyGredientAgent(env.observation_space.shape[0], env.action_space.n, 0.99, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000, Reward: 72.40082946040363, Steps: 199, Last 100 rewards Mean:76.2297351076961, 500: 67.21628717272448646\r"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "last_500_rewards, last_100_rawards = np.zeros((500,)), np.zeros((100,))\n",
    "for episode in range(episodes):\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        a = pg_agent.choose_action(obs)\n",
    "        obs_, r, done, _ = env.step(a)\n",
    "        x, vel, angle, angle_vel = obs_\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold-0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(angle))/env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "        pg_agent.store_experience(obs, a, r)\n",
    "        if done:\n",
    "            pg_agent.train()\n",
    "            pg_agent.write_rewards(episode, total_reward)\n",
    "            break\n",
    "        obs = obs_\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "    last_100_rawards[episode%100] = total_reward\n",
    "    last_500_rewards[episode%500] = total_reward\n",
    "    print('Episode {}, Reward: {}, Steps: {}, Last 100 rewards Mean:{}, 500: {}'.format(episode+1, total_reward, steps,\n",
    "                                                                                        last_100_rawards.mean(), \n",
    "                                                                                        last_500_rewards.mean()), end='\\r')\n",
    "env.close()\n",
    "pg_agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Steps: 199\r"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "pg_agent.load()\n",
    "for episode in range(episodes):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        a = pg_agent.choose_action(obs)\n",
    "        obs_, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            break\n",
    "        steps += 1\n",
    "        obs = obs_\n",
    "    print('Episode {}, Steps: {}'.format(episode+1, steps), \n",
    "          end='\\r')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13812), started 0:10:12 ago. (Use '!kill 13812' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fd379b438d69b18c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fd379b438d69b18c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir './logs/pg_agent/cartpole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
